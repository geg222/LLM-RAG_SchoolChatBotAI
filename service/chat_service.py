import requests
from service.rag_service import get_retriever
from service.conversation_service import get_conversation_service
from service.intent_classifier import get_intent_classifier
from core.logger import logger
from langchain_openai import ChatOpenAI
from pydantic import SecretStr
import os
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_community.chat_message_histories import ChatMessageHistory
from datetime import datetime

_llm_cache = {}
store = {}

def get_session_history(session_id: str):
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

def _clean_markdown_format(text):
    """
    ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì„ ì •ë¦¬í•˜ì—¬ ê¹”ë”í•œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    import re
    
    # **í…ìŠ¤íŠ¸** â†’ í…ìŠ¤íŠ¸ (êµµì€ ê¸€ì”¨ ì œê±°)
    text = re.sub(r'\*\*(.*?)\*\*', r'\1', text)
    
    # [í…ìŠ¤íŠ¸](ë§í¬) â†’ í…ìŠ¤íŠ¸ (ë§í¬ í˜•ì‹ ì •ë¦¬)
    text = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', text)
    
    # ë¶ˆí•„ìš”í•œ ë§ˆí¬ë‹¤ìš´ ê¸°í˜¸ ì œê±°
    text = re.sub(r'#{1,6}\s+', '', text)  # í—¤ë”© ì œê±°
    text = re.sub(r'\*\s+', 'â€¢ ', text)    # ë¦¬ìŠ¤íŠ¸ ê¸°í˜¸ ì •ë¦¬
    
    # ì—°ì†ëœ ê³µë°± ì •ë¦¬
    text = re.sub(r'\n\s*\n', '\n\n', text)
    
    return text.strip()

def get_llm(model='gpt-4o'):
    if model not in _llm_cache:
        api_key = os.getenv("OPENAI_API_KEY")
        if api_key is not None:
            api_key = SecretStr(api_key)
        _llm_cache[model] = ChatOpenAI(model=model, api_key=api_key)
    return _llm_cache[model]

def get_ai_response(user_message, session_id="default_session"):
    """
    Gets and processes AI response using enhanced RAG approach with conversation context and intent classification.
    """
    try:
        # Get services
        llm = get_llm()
        conversation_service = get_conversation_service()
        intent_classifier = get_intent_classifier()
        
        # Get session history
        session_history = get_session_history(session_id)
        chat_history = session_history.messages
        
        # Get conversation context
        conversation_context = conversation_service.get_context(session_id, max_turns=3)
        
        # Classify user intent
        intent, confidence = intent_classifier.classify_intent(user_message)
        logger.info(f"Intent classified: {intent} (confidence: {confidence:.2f})")
        
        # Get current date
        current_date_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        # Retrieve documents using the enhanced retriever
        retrieved_docs = get_retriever(user_message)
        logger.info(f"Retrieved {len(retrieved_docs)} documents for the query.")

        if not retrieved_docs:
            logger.warning("No documents were retrieved.")
            return "ì •í™•í•œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ğŸ˜…\n\në‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ë‹¤ì‹œ ë¬¼ì–´ë³´ì‹œê±°ë‚˜, í•œì„±ëŒ€í•™êµ í•™ìƒì§€ì›ì„¼í„°ì— ì§ì ‘ ë¬¸ì˜í•´ë³´ì„¸ìš”!"
        else:
            for i, doc in enumerate(retrieved_docs[:3]):
                logger.info(f"Retrieved Doc {i+1}: {doc.metadata.get('title', 'N/A')}")

        # Create context from documents
        context = "\n\n".join([doc.page_content for doc in retrieved_docs])

        # Create enhanced system prompt with intent-specific guidance
        base_prompt = intent_classifier.get_intent_specific_prompt(intent, user_message)
        
        system_prompt = (
            f"{base_prompt} "
            "ë°˜ë“œì‹œ í•œì„±ëŒ€í•™êµ ê³µì‹ ê³µì§€ì‚¬í•­ì˜ URLì„ í¬í•¨í•´ì„œ, í•™ìƒì´ ë°”ë¡œ í´ë¦­í•  ìˆ˜ ìˆë„ë¡ ì•ˆë‚´í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤. "
            "\n\n"
            "ë‹µë³€ í˜•ì‹ì„ ë‹¤ìŒê³¼ ê°™ì´ ì •í™•íˆ ì§€ì¼œì„œ ë‹µë³€í•´ì¤˜:\n"
            "\n"
            "ì—¬ê¸° [ì§ˆë¬¸ í‚¤ì›Œë“œ]ì— ëŒ€í•œ ê³µì§€ì‚¬í•­ì´ ìˆìŠµë‹ˆë‹¤!\n"
            "\n"
            "1. ê³µì§€ì‚¬í•­ ì œëª©: [ì œëª©]\n"
            "\n"
            "2. ì£¼ìš” ë‚´ìš© ìš”ì•½: [ë‚´ìš© ìš”ì•½]\n"
            "\n"
            "3. ì¤‘ìš” ì •ë³´: [ì‹ ì²­ê¸°ê°„, ì ‘ìˆ˜ê¸°ê°„, ëª¨ì§‘ê¸°ê°„, ì•ˆë‚´ì‚¬í•­ ë“± ê³µì§€ì‚¬í•­ì— í¬í•¨ëœ ì¤‘ìš” ì •ë³´]\n"
            "\n"
            "4. ì‹ ì²­ ë°©ë²•: [ì‹ ì²­/ì ‘ìˆ˜ ë°©ë²•ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ í¬í•¨]\n"
            "\n"
            "5. ê³µì‹ ë§í¬\n[ë§í¬ URLë§Œ ì •í™•íˆ ì…ë ¥]\n"
            "\n"
            "[ë§ˆë¬´ë¦¬ ë©˜íŠ¸]\n"
            "\n"
            "âš ï¸ ì¤‘ìš”í•œ ê·œì¹™: "
            "â€¢ ë§ˆí¬ë‹¤ìš´ í˜•ì‹(**êµµì€ ê¸€ì”¨**)ì„ ì‚¬ìš©í•˜ì§€ ë§ê³  ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ë‹µë³€í•´ì¤˜. "
            "â€¢ ì¤„ë°”ê¿ˆì„ ì ì ˆíˆ ì‚¬ìš©í•´ì„œ ê°€ë…ì„±ì„ ë†’ì—¬ì¤˜. "
            "â€¢ ê³µì§€ì‚¬í•­ì— ì‹ ì²­ê¸°ê°„ì´ ì—†ìœ¼ë©´ '3. ì¤‘ìš” ì •ë³´'ì— ë‹¤ë¥¸ ì¤‘ìš” ì •ë³´ë¥¼ í¬í•¨í•´ì¤˜. "
            "â€¢ ì‹ ì²­ ë°©ë²•ì´ ì—†ìœ¼ë©´ í•´ë‹¹ í•­ëª©ì„ ìƒëµí•´ì¤˜. "
            "â€¢ ê²€ìƒ‰ëœ ë¬¸ì„œ ì¤‘ì—ì„œ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ê³µì§€ì‚¬í•­ì´ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ë‹µë³€í•´ì¤˜! "
            "â€¢ ì œëª©ì— ì •í™•íˆ ì¼ì¹˜í•˜ì§€ ì•Šì•„ë„ ë‚´ìš©ì´ ê´€ë ¨ë˜ë©´ ë‹µë³€í•´ì¤˜. "
            "â€¢ ì˜ˆë¥¼ ë“¤ì–´ 'íŠ¸ë™ë³€ê²½'ì„ ë¬¼ì–´ë³´ë©´ ì œëª©ì— 'íŠ¸ë™ë³€ê²½'ì´ í¬í•¨ëœ ê³µì§€ì‚¬í•­ì„ ì°¾ì•„ì„œ ë‹µë³€í•´ì¤˜. "
            "â€¢ 5. ê³µì‹ ë§í¬ì—ëŠ” ë°˜ë“œì‹œ https://ë¡œ ì‹œì‘í•˜ëŠ” ì™„ì „í•œ URLë§Œ ì…ë ¥í•´ì¤˜. "
            "ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì „í˜€ ê´€ë ¨ì´ ì—†ì„ ë•ŒëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë‹µë³€í•´ì¤˜:\n"
            "\n"
            "ì •í™•í•œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ğŸ˜…\n\n"
            "[ì§ˆë¬¸ ë‚´ìš©]ì— ëŒ€í•œ ê³µì§€ì‚¬í•­ì€ í˜„ì¬ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"
            "í•œì„±ëŒ€í•™êµ (â˜760-4219)ì— ì§ì ‘ ë¬¸ì˜í•˜ê±°ë‚˜, "
            "í•œì„±ëŒ€í•™êµ ê³µì‹ í™ˆí˜ì´ì§€(https://www.hansung.ac.kr)ì—ì„œ ê³µì§€ì‚¬í•­ì„ í™•ì¸í•´ë³´ì„¸ìš”!\n\n"
            "--- ê²€ìƒ‰ëœ ê´€ë ¨ ê³µì§€ì‚¬í•­ ---\n"
            "{context}"
            "--- ë ---\n"
        )
        
        qa_prompt = ChatPromptTemplate.from_messages(
            [
                ("system", system_prompt),
                MessagesPlaceholder("chat_history"),
                ("human", "{input}"),
            ]
        )
        
        # Call the LLM directly
        response = llm.invoke(qa_prompt.format_messages(
            input=user_message,
            chat_history=chat_history,
            context=context,
            current_date=current_date_str
        ))
        
        ai_response = response.content

        # ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        conversation_service.add_to_history(session_id, user_message, ai_response)
        
        # ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ì •ë¦¬ (ë¶ˆí•„ìš”í•œ ** ì œê±°)
        ai_response = _clean_markdown_format(ai_response)

        # Add messages to session history
        from langchain_core.messages import HumanMessage, AIMessage
        session_history.add_user_message(user_message)
        session_history.add_ai_message(ai_response)

        if not ai_response.strip():
            logger.warning("LLM response was empty.")
            return "ì°¾ì€ ì •ë³´ê°€ ë¶€ì¡±í•´ì„œ ì •í™•í•œ ë‹µë³€ì„ ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤. ğŸ˜…\n\në‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ë‹¤ì‹œ ë¬¼ì–´ë³´ì‹œê±°ë‚˜, í•œì„±ëŒ€í•™êµ í•™ìƒì§€ì›ì„¼í„°ì— ì§ì ‘ ë¬¸ì˜í•´ë³´ì„¸ìš”!"

        return ai_response

    except Exception as e:
        logger.error(f"Error during AI response generation: {e}")
        logger.error(f"Error type: {type(e)}")
        import traceback
        logger.error(f"Full traceback: {traceback.format_exc()}")
        return "ì±—ë´‡ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”!" 